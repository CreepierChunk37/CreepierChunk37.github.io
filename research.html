<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Experience - Yitong Qiu's Academic Homepage</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Yitong Qiu</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="index.html#about">About Me</a></li>
                <li><a href="index.html#education">Education</a></li>
                <li><a href="index.html#skills">Skills</a></li>
                <li><a href="index.html#awards">Awards</a></li>
                <li><a href="index.html#extracurricular">Extracurricular</a></li>
                <li><a href="index.html#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="experience">
            <h2>Research Experience</h2>
            <ul>
                <li>
                    <h3>Memorization and Generalization Behavior of Diffusion Models</h3>
                    <p>Georgia Institute of Technology</p>
                    <p>Undergraduate Researcher, Supervisor: Molei Tao</p>
                    <p>09/2024 - Present</p>
                    <ul>
                        <li>Conducted extensive early-stage experiments that identified distinctive spatial preferences in diffusion model generalization, providing critical direction for subsequent theoretical analysis and inspiring the development of rigorous mathematical frameworks to characterize inductive bias and generalization error in generative modeling.</li>
                        <li>Departing from previous works that largely assume infinite data dimensions, our research is conducted in a fixed-dimensional setting. We developed a novel analytical framework to compute critical expectations—arising within the gradient flow induced by gradient descent—involving ReLU-activated neurons and their derivatives. This method combines the generalized Mehler's formula with an asymptotic analysis of Hermite polynomial series expansions.</li>
                        <li>We apply the above framework to understand the memorization and generalization problems in generative models. By analyzing the sampling dynamics of diffusion models, we establish corresponding expressions for several key quantities that characterize this dynamic process for the first time. These results directly relate to the model's inductive bias and generalization error, providing a foundation for theoretically understanding the behavior of generative models.</li>
                    </ul>
                </li>
                <li>
                    <h3>Invariant Measure of High-Dimensional Stochastic Differential Equations</h3>
                    <p>University of Science and Technology of China</p>
                    <p>Leader, Supervisor: Jianliang Zhai</p>
                    <p>09/2024 - Present</p>
                    <ul>
                        <li>Conducted literature review on Random Perturbations of Dynamical Systems and utilized quasipotential to extend theorems in uniform large deviations and metastability.</li>
                        <li>Developed a novel constructive analytical method for high-dimensional stochastic differential equations with locally Lipschitz coefficients. Our approach employs a multi-stage control strategy using shifted Legendre polynomials as control inputs, enabling the explicit construction of admissible paths connecting any two nearby points.</li>
                        <li>Rigorously proved the continuity of the quasipotential for this broader class of systems. Our result successfully overcomes the long-standing restriction in classical Freidlin-Wentzell theory requiring globally bounded coefficients, thereby significantly extending the theory's scope of applicability.</li>
                        <li>Preparing manuscript on the continuity of quasipotential under local Lipschitz conditions and its applications. An early <a href="Quasipotential-7.pdf" target="_blank" rel="noopener">Draft</a></li>
                    </ul>
                </li>
                <li>
                    <h3>Unsupervised Learning Based on Diffusion Models</h3>
                    <p>University of Science and Technology of China</p>
                    <p>Undergraduate Researcher, Supervisor: Jianbin Tan (Postdoc, Duke University), Yukang Jiang (Postdoc, UNC), Xue-qin Wang (USTC)</p>
                    <p>03/2024 - 06/2025</p>
                    <ul>
                        <li>Conducted in-depth literature review on Stochastic Differential Equations (SDEs) and their applications in diffusion models, gaining theoretical insights into generative modeling and probabilistic frameworks.</li>
                        <li>Developed Python programs to reproduce studies and integrate diffusion models into dimensionality reduction. We proposed several framework combining diffusion model and distribution matching techniques.</li>
                    </ul>
                </li>
            </ul>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Yitong Qiu. All rights reserved.</p>
    </footer>
</body>
</html>
